{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data\n",
    "import pandas as ps\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "import math\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>state</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Edinburgh peeps is it sunny?? #weather</td>\n",
       "      <td>NaN</td>\n",
       "      <td>birmingham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>SEEVERE T’STORM WARNING FOR TROUSDALE,  NORTHW...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nashville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>@Agilis1 sport or traditional climbing? Thats ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Midwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>#WEATHER: 10:53 am : 63.0F. Feels 61F. 30.07% ...</td>\n",
       "      <td>tennessee</td>\n",
       "      <td>Nashville, TN, USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>We used to use umbrellas to face the bad weath...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet      state  \\\n",
       "0   4             Edinburgh peeps is it sunny?? #weather        NaN   \n",
       "1   5  SEEVERE T’STORM WARNING FOR TROUSDALE,  NORTHW...        NaN   \n",
       "2   7  @Agilis1 sport or traditional climbing? Thats ...        NaN   \n",
       "3   8  #WEATHER: 10:53 am : 63.0F. Feels 61F. 30.07% ...  tennessee   \n",
       "4  12  We used to use umbrellas to face the bad weath...        NaN   \n",
       "\n",
       "             location  \n",
       "0          birmingham  \n",
       "1           Nashville  \n",
       "2             Midwest  \n",
       "3  Nashville, TN, USA  \n",
       "4             Houston  "
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = 'data/train.csv'\n",
    "test_path = 'data/test.csv'\n",
    "\n",
    "train = ps.read_csv(train_path)\n",
    "test = ps.read_csv(test_path)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "### use twokenized data\n",
    "twokenized_train = ps.read_fwf('train_tokenized.txt',header=None)\n",
    "train['tweet'] = twokenized_train[0]\n",
    "twokenized_test = []\n",
    "twokenized_test = ps.read_fwf('ark-tweet-nlp-0.3.2/test_tokenized.txt',header=None)\n",
    "test['tweet'] = twokenized_test[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.00239629 0.00607721 0.00585773 0.00516153 0.00530749 0.00455613\n",
      " 0.00403561 0.00372277 0.00365302 0.0035049  0.00322836 0.00309209\n",
      " 0.00304916 0.00288514 0.00286447 0.0027701  0.00268783 0.00258244\n",
      " 0.00252332 0.00245024 0.00230378 0.0022814  0.00225264 0.00217269]\n",
      "0.0814163503053813\n",
      "[36.37564981 21.60272289 21.25680199 20.34973173 20.2001371  18.70758617\n",
      " 17.67412347 16.92983807 16.75980384 16.40649894 15.74423752 15.41353904\n",
      " 15.30417657 14.89687142 14.83510839 14.583515   14.37185378 14.08101022\n",
      " 13.91968934 13.7169637  13.29977222 13.25120283 13.15284549 12.93629008]\n"
     ]
    }
   ],
   "source": [
    "svd = TruncatedSVD(n_components=24, n_iter=7, random_state=42)\n",
    "svd.fit(X)\n",
    "print(svd.explained_variance_ratio_)\n",
    "print(svd.explained_variance_ratio_.sum())\n",
    "print(svd.singular_values_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
       "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=10000, strip_accents='unicode', analyzer='word', ngram_range=(1, 3))\n",
    "tfidf.fit(train['tweet'])\n",
    "\n",
    "# tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 9226)\t0.12284467665531285\n",
      "  (0, 7717)\t0.49272607970403354\n",
      "  (0, 7664)\t0.23197017089862015\n",
      "  (0, 6387)\t0.5550614714805855\n",
      "  (0, 4435)\t0.41818562136462034\n",
      "  (0, 4297)\t0.17465513595134888\n",
      "  (0, 4194)\t0.37509243740383125\n",
      "  (0, 4139)\t0.18479402441684495\n",
      "  (1, 9071)\t0.2875785522726091\n",
      "  (1, 9070)\t0.24349207781313464\n",
      "  (1, 7785)\t0.44802610636675827\n",
      "  (1, 7421)\t0.4593065751948661\n",
      "  (1, 5895)\t0.4533523459358235\n",
      "  (1, 3783)\t0.35288594297213616\n",
      "  (1, 2907)\t0.14590223115220577\n",
      "  (1, 1495)\t0.3081817044271085\n",
      "  (2, 9491)\t0.23392016992769885\n",
      "  (2, 9226)\t0.06712521262014724\n",
      "  (2, 8624)\t0.183937243742933\n",
      "  (2, 8469)\t0.09034159681793655\n",
      "  (2, 8040)\t0.29738261923793863\n",
      "  (2, 7909)\t0.0732765577740799\n",
      "  (2, 7908)\t0.2610024177995806\n",
      "  (2, 7901)\t0.28344099758955604\n",
      "  (2, 7869)\t0.13890727335792646\n",
      "  :\t:\n",
      "  (42154, 1074)\t0.204861108145556\n",
      "  (42154, 764)\t0.18570914113525172\n",
      "  (42154, 195)\t0.127478883775513\n",
      "  (42155, 9226)\t0.17466838700620282\n",
      "  (42155, 8748)\t0.24919660397411442\n",
      "  (42155, 7100)\t0.33929116230781453\n",
      "  (42155, 6806)\t0.34288993979836374\n",
      "  (42155, 5637)\t0.41522803043254664\n",
      "  (42155, 3899)\t0.1188001965513267\n",
      "  (42155, 3851)\t0.3984792428322975\n",
      "  (42155, 3783)\t0.3306081028044477\n",
      "  (42155, 1966)\t0.28454722232021157\n",
      "  (42155, 1082)\t0.35215220126905533\n",
      "  (42155, 904)\t0.12290842265212869\n",
      "  (42156, 8774)\t0.4076298553737273\n",
      "  (42156, 8773)\t0.3237294907600553\n",
      "  (42156, 7664)\t0.1582745537914273\n",
      "  (42156, 6282)\t0.16646422759460514\n",
      "  (42156, 5812)\t0.37962167777304917\n",
      "  (42156, 5803)\t0.1972083178963222\n",
      "  (42156, 5161)\t0.3450399444040803\n",
      "  (42156, 4528)\t0.3676961500061506\n",
      "  (42156, 4488)\t0.16456594095975025\n",
      "  (42156, 3673)\t0.40135035804621083\n",
      "  (42156, 3667)\t0.2255108083872208\n"
     ]
    }
   ],
   "source": [
    "X = tfidf.transform(train['tweet'])\n",
    "y = np.array(train.iloc[:,4:])\n",
    "X_test = tfidf.transform(test['tweet'].values.astype('U')) \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "print(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor()\n",
    "\n",
    "param_grid = {'max_depth': [None], 'min_samples_split' :[3, 10], 'min_samples_leaf' : [3, 10],\n",
    "              'criterion':['mse']}\n",
    "\n",
    "clf = GridSearchCV(clf, param_grid=param_grid, cv=2, verbose=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "[CV] criterion=mse, max_depth=None, min_samples_leaf=3, min_samples_split=3 \n",
      "[CV]  criterion=mse, max_depth=None, min_samples_leaf=3, min_samples_split=3, score=0.5293397477735104, total= 1.2min\n",
      "[CV] criterion=mse, max_depth=None, min_samples_leaf=3, min_samples_split=3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.2min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=mse, max_depth=None, min_samples_leaf=3, min_samples_split=3, score=0.5254830720937576, total= 1.3min\n",
      "[CV] criterion=mse, max_depth=None, min_samples_leaf=3, min_samples_split=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.5min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=mse, max_depth=None, min_samples_leaf=3, min_samples_split=10, score=0.5342758554858101, total= 1.1min\n",
      "[CV] criterion=mse, max_depth=None, min_samples_leaf=3, min_samples_split=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  3.6min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=mse, max_depth=None, min_samples_leaf=3, min_samples_split=10, score=0.5307195305996409, total= 1.0min\n",
      "[CV] criterion=mse, max_depth=None, min_samples_leaf=10, min_samples_split=3 \n",
      "[CV]  criterion=mse, max_depth=None, min_samples_leaf=10, min_samples_split=3, score=0.5137308011872684, total=  33.5s\n",
      "[CV] criterion=mse, max_depth=None, min_samples_leaf=10, min_samples_split=3 \n",
      "[CV]  criterion=mse, max_depth=None, min_samples_leaf=10, min_samples_split=3, score=0.5140803302643739, total=  33.0s\n",
      "[CV] criterion=mse, max_depth=None, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=mse, max_depth=None, min_samples_leaf=10, min_samples_split=10, score=0.5169491409014524, total=  33.8s\n",
      "[CV] criterion=mse, max_depth=None, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=mse, max_depth=None, min_samples_leaf=10, min_samples_split=10, score=0.5124272792454402, total=  33.3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  6.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [None], 'min_samples_split': [3, 10], 'min_samples_leaf': [3, 10], 'criterion': ['mse']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=4)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_val)\n",
    "# Truncate predictions between 0 and 1\n",
    "y_pred[y_pred < 0] = 0\n",
    "y_pred[y_pred > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16728293590703416"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting xgboost\n",
      "  Using cached https://files.pythonhosted.org/packages/83/3a/8570f4e8e19acd3a5a75abc920964182a4b64db2ee0f041fb77b48447c6b/xgboost-0.72.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files/directories in C:\\Users\\Jesse\\AppData\\Local\\Temp\\pip-install-jnybtsz_\\xgboost\\pip-egg-info (from PKG-INFO)\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'xgboost'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d04e072ae738>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmultioutput\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mMultiOutputRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'pip install xgboost'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mxgboost\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mXGBRegressor\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'xgboost'"
     ]
    }
   ],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "!pip install xgboost\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multioutputregressor = MultiOutputRegressor(XGBRegressor(objective='reg:linear')).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mor_pred = multioutputregressor.predict(X_val)\n",
    "# Truncate\n",
    "mor_pred[mor_pred > 1] = 1\n",
    "mor_pred[mor_pred < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = math.sqrt(mean_squared_error(y_val, mor_pred))\n",
    "rmse # 0.16388598799139095"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Ridge()\n",
    "\n",
    "param_grid = {'alpha': [25, 10, 5, 2.5, 1, 0.75, 0.5, 0.25, 0.1, 0.05, 0.025, 0.01, 0.001]}\n",
    "\n",
    "clf = GridSearchCV(clf, param_grid=param_grid, cv=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 13 candidates, totalling 26 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed: 19.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [25, 10, 5, 2.5, 1, 0.75, 0.5, 0.25, 0.1, 0.05, 0.025, 0.01, 0.001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_val)\n",
    "# Truncate predictions between 0 and 1\n",
    "y_pred[y_pred < 0] = 0\n",
    "y_pred[y_pred > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15363337043645406"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset for S, W and K classifications.\n",
    "y_train_S = y_train[:,0:5]\n",
    "y_train_W = y_train[:, 5:9]\n",
    "y_train_K = y_train[:, 9:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 13 candidates, totalling 26 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:  4.4min finished\n"
     ]
    }
   ],
   "source": [
    "# Predict Sentiment\n",
    "clf.fit(X_train, y_train_S)\n",
    "y_pred_S = clf.predict(X_val)\n",
    "# Truncate predictions between 0 and 1\n",
    "y_pred_S[y_pred_S < 0] = 0\n",
    "y_pred_S[y_pred_S > 1] = 1\n",
    "# rmse = math.sqrt(mean_squared_error(y_val[:,0:5], y_pred_S))\n",
    "# rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 13 candidates, totalling 26 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:  4.2min finished\n"
     ]
    }
   ],
   "source": [
    "# Predict When\n",
    "clf.fit(X_train, y_train_W)\n",
    "y_pred_W = clf.predict(X_val)\n",
    "# Truncate predictions between 0 and 1\n",
    "y_pred_W[y_pred_W < 0] = 0\n",
    "y_pred_W[y_pred_W > 1] = 1\n",
    "# rmse = math.sqrt(mean_squared_error(y_val[:,5:9], y_pred_W))\n",
    "# rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 13 candidates, totalling 26 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed: 24.9min finished\n"
     ]
    }
   ],
   "source": [
    "# Predict Kind\n",
    "clf.fit(X_train, y_train_K)\n",
    "y_pred_K = clf.predict(X_val)\n",
    "# Truncate predictions between 0 and 1\n",
    "y_pred_K[y_pred_K < 0] = 0\n",
    "y_pred_K[y_pred_K > 1] = 1\n",
    "# rmse = math.sqrt(mean_squared_error(y_val[:,9:23], y_pred_K))\n",
    "# rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ensemble of 24 classifiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset for S, W and K classifications.\n",
    "#for sentiment:https://www.ravikiranj.net/posts/2012/code/how-build-twitter-sentiment-analyzer/\n",
    "#tf-idf and \n",
    "y_train_S1 = y_train[:,0]\n",
    "y_train_S2 = y_train[:,1]\n",
    "y_train_S3 = y_train[:,2]\n",
    "y_train_S4 = y_train[:,3]\n",
    "y_train_S5 = y_train[:,4]\n",
    "y_train_W1 = y_train[:, 5]\n",
    "y_train_W2 = y_train[:, 6]\n",
    "y_train_W3 = y_train[:, 7]\n",
    "y_train_W4 = y_train[:, 8]\n",
    "y_train_K1 = y_train[:, 9]\n",
    "y_train_K2 = y_train[:, 10]\n",
    "y_train_K3 = y_train[:, 11]\n",
    "y_train_K4 = y_train[:, 12]\n",
    "y_train_K5 = y_train[:, 13]\n",
    "y_train_K6 = y_train[:, 14]\n",
    "y_train_K7 = y_train[:, 15]\n",
    "y_train_K8 = y_train[:, 16]\n",
    "y_train_K9 = y_train[:, 17]\n",
    "y_train_K10 = y_train[:, 18]\n",
    "y_train_K11 = y_train[:, 19]\n",
    "y_train_K12 = y_train[:, 20]\n",
    "y_train_K13 = y_train[:, 21]\n",
    "y_train_K14 = y_train[:, 22]\n",
    "y_train_K15 = y_train[:, 23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#classifiers\n",
    "clf1 = \n",
    "clf2 =\n",
    "clf3 = \n",
    "clf4 = \n",
    "clf5 = \n",
    "clf6 = \n",
    "clf7 = \n",
    "clf8 = \n",
    "clf9 = \n",
    "clf10 = \n",
    "clf11 = \n",
    "clf12 = \n",
    "clf13 = \n",
    "clf14 = \n",
    "clf15 = \n",
    "clf16 = \n",
    "clf17 = \n",
    "clf18 = \n",
    "clf19 = \n",
    "clf20 = \n",
    "clf21 = \n",
    "clf22 = \n",
    "clf23 = \n",
    "clf24 = \n",
    "classifiers = [clf1,clf2,clf3,clf4,clf5,clf5,clf6,clf7,clf8,clf9,clf10,clf11,clf12,clf13,clf14,clf15,clf16,clf17,clf18,clf19,]classifiers = [clf1,clf2,clf3,clf4,clf5,clf5,clf6,clf7,clf8,clf9,clf10,clf11,clf12,clf13,clf14,clf15,clf16,clf17,clf18,clf19,]classifiers = [clf1,clf2,clf3,clf4,clf5,clf5,clf6,clf7,clf8,clf9,clf10,clf11,clf12,clf13,clf14,clf15,clf16,clf17,clf18,clf19,clf20,clf21,clf22,clf23,clf24]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = clf.predict(X_test)\n",
    "# Truncate predictions between 0 and 1\n",
    "test_prediction[test_prediction < 0] = 0\n",
    "test_prediction[test_prediction > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.array(np.hstack([np.matrix(test['id']).T, test_prediction])) \n",
    "col = '%i,' + '%f,'*23 + '%f'\n",
    "np.savetxt('data/output.txt', prediction,col, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.14432603441695394\n"
     ]
    }
   ],
   "source": [
    "print('Train error: {0}'.format(np.sqrt(np.sum(np.array(np.array(clf.predict(X))-y)**2)/ (X.shape[0]*24.0))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
