{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read the data\n",
    "import pandas as ps\n",
    "import numpy as np\n",
    "from sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import cross_val_score, train_test_split, GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import math\n",
    "import nltk\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "import re\n",
    "\n",
    "# nltk.download()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>tweet</th>\n",
       "      <th>state</th>\n",
       "      <th>location</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>4</td>\n",
       "      <td>Edinburgh peeps is it sunny?? #weather</td>\n",
       "      <td>NaN</td>\n",
       "      <td>birmingham</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>SEEVERE T’STORM WARNING FOR TROUSDALE,  NORTHW...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Nashville</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7</td>\n",
       "      <td>@Agilis1 sport or traditional climbing? Thats ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Midwest</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>#WEATHER: 10:53 am : 63.0F. Feels 61F. 30.07% ...</td>\n",
       "      <td>tennessee</td>\n",
       "      <td>Nashville, TN, USA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>12</td>\n",
       "      <td>We used to use umbrellas to face the bad weath...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Houston</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id                                              tweet      state  \\\n",
       "0   4             Edinburgh peeps is it sunny?? #weather        NaN   \n",
       "1   5  SEEVERE T’STORM WARNING FOR TROUSDALE,  NORTHW...        NaN   \n",
       "2   7  @Agilis1 sport or traditional climbing? Thats ...        NaN   \n",
       "3   8  #WEATHER: 10:53 am : 63.0F. Feels 61F. 30.07% ...  tennessee   \n",
       "4  12  We used to use umbrellas to face the bad weath...        NaN   \n",
       "\n",
       "             location  \n",
       "0          birmingham  \n",
       "1           Nashville  \n",
       "2             Midwest  \n",
       "3  Nashville, TN, USA  \n",
       "4             Houston  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_path = 'data/train.csv'\n",
    "test_path = 'data/test.csv'\n",
    "\n",
    "train = ps.read_csv(train_path)\n",
    "test = ps.read_csv(test_path)\n",
    "\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### use twokenized data\n",
    "twokenized_train = ps.read_fwf('train_tokenized.txt',header=None)\n",
    "train['tweet'] = twokenized_train[0]\n",
    "twokenized_test = []\n",
    "twokenized_test = ps.read_fwf('ark-tweet-nlp-0.3.2/test_tokenized.txt',header=None)\n",
    "test['tweet'] = twokenized_test[0]\n",
    "\n",
    "#initialize stemmer\n",
    "stemmer = SnowballStemmer('english')\n",
    "### remove stopwords\n",
    "### don't know if this is useful when using ngrams\n",
    "ps.options.mode.chained_assignment = None\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.add(\"rt\")\n",
    "stop_words.add(\"{\")\n",
    "stop_words.add(\"}\")\n",
    "stop_words.add(\"link\")\n",
    "stop_words.add(\"google\")\n",
    "stop_words.add(\"facebook\")\n",
    "stop_words.add(\"twitter\")\n",
    "for i in range(len(train['tweet'])):\n",
    "    train['tweet'][i] = [x.lower() for x in train['tweet'][i].split() if not x in stop_words] # to lower case\n",
    "    train['tweet'][i] = \" \".join(train['tweet'][i])\n",
    "    train['tweet'][i] = re.sub(r'(\\w)\\1{2,}', r'\\1\\1',test)\n",
    "    train['tweet'][i] = stemmer.stem(train['tweet'][i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'disfunct'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "freakin killin audio ustream ! :/ freez\n",
      "freakin killin audio ustream ! :/ freez\n"
     ]
    }
   ],
   "source": [
    "print(train['tweet'][100])\n",
    "print(stemmer.stem(train['tweet'][100]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=10000, min_df=1,\n",
       "        ngram_range=(1, 7), norm='l2', preprocessor=None, smooth_idf=True,\n",
       "        stop_words=None, strip_accents='unicode', sublinear_tf=False,\n",
       "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
       "        vocabulary=None)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf = TfidfVectorizer(max_features=10000, strip_accents='unicode', analyzer='word', ngram_range=(1, 7))\n",
    "tfidf.fit(train['tweet'])\n",
    "\n",
    "# tfidf.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.    0.    0.201 ... 0.    0.    0.   ]\n",
      " [0.    0.    0.205 ... 0.    0.    0.   ]\n",
      " [0.    0.2   0.    ... 1.    0.    0.   ]\n",
      " ...\n",
      " [0.    0.    0.    ... 0.    0.    0.   ]\n",
      " [0.327 0.172 0.    ... 0.    0.    0.   ]\n",
      " [0.222 0.385 0.207 ... 0.273 0.    0.   ]]\n"
     ]
    }
   ],
   "source": [
    "X = tfidf.transform(train['tweet'])\n",
    "y = np.array(train.iloc[:,4:])\n",
    "X_test = tfidf.transform(test['tweet'].values.astype('U')) \n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y)\n",
    "\n",
    "print(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LSA to training, validation and test data generated by TF-IDF to generate the new datasets.\n",
    "\n",
    "def LSA(X_train, X_val, X_test):\n",
    "    lsa = TruncatedSVD(n_components=100)\n",
    "    lsa.fit(X_train, y_train)\n",
    "    X_train = lsa.transform(X_train)\n",
    "    X_val = lsa.transform(X_val)\n",
    "    X_test = lsa.transform(X_test)\n",
    "\n",
    "    return X_train, X_val, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply LDA directly to the tweet data to generate training, validation and test data.\n",
    "\n",
    "def LDA():\n",
    "    tf_vectorizer = CountVectorizer(max_features=10000, strip_accents='unicode', analyzer='word', ngram_range=(1, 3))\n",
    "    tf = tf_vectorizer.fit_transform(train['tweet'])\n",
    "    lda = LatentDirichletAllocation(n_components = 200, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)   \n",
    "    tf = lda.transform(tf)\n",
    "\n",
    "    X_test = tf_vectorizer.transform(test['tweet'].values.astype('U'))\n",
    "    X_test = lda.transform(X_test)\n",
    "\n",
    "    X_train, X_val, y_train, y_val = train_test_split(tf, y)\n",
    "    \n",
    "    return X_train, X_val, y_train, y_val, X_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestRegressor()\n",
    "\n",
    "param_grid = {'max_depth': [None], 'min_samples_split' :[3, 10], 'min_samples_leaf' : [3, 10],\n",
    "              'criterion':['mse']}\n",
    "\n",
    "clf = GridSearchCV(clf, param_grid=param_grid, cv=2, verbose=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 4 candidates, totalling 8 fits\n",
      "[CV] criterion=mse, max_depth=None, min_samples_leaf=3, min_samples_split=3 \n",
      "[CV]  criterion=mse, max_depth=None, min_samples_leaf=3, min_samples_split=3, score=0.510582705260425, total=  40.4s\n",
      "[CV] criterion=mse, max_depth=None, min_samples_leaf=3, min_samples_split=3 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:   40.6s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=mse, max_depth=None, min_samples_leaf=3, min_samples_split=3, score=0.514413202634155, total=  40.1s\n",
      "[CV] criterion=mse, max_depth=None, min_samples_leaf=3, min_samples_split=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  1.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=mse, max_depth=None, min_samples_leaf=3, min_samples_split=10, score=0.5191648494986347, total=  35.7s\n",
      "[CV] criterion=mse, max_depth=None, min_samples_leaf=3, min_samples_split=10 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:  1.9min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  criterion=mse, max_depth=None, min_samples_leaf=3, min_samples_split=10, score=0.5204157201319333, total=  36.5s\n",
      "[CV] criterion=mse, max_depth=None, min_samples_leaf=10, min_samples_split=3 \n",
      "[CV]  criterion=mse, max_depth=None, min_samples_leaf=10, min_samples_split=3, score=0.49900043369778674, total=  19.6s\n",
      "[CV] criterion=mse, max_depth=None, min_samples_leaf=10, min_samples_split=3 \n",
      "[CV]  criterion=mse, max_depth=None, min_samples_leaf=10, min_samples_split=3, score=0.5025388783991691, total=  19.0s\n",
      "[CV] criterion=mse, max_depth=None, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=mse, max_depth=None, min_samples_leaf=10, min_samples_split=10, score=0.5004914024755611, total=  19.0s\n",
      "[CV] criterion=mse, max_depth=None, min_samples_leaf=10, min_samples_split=10 \n",
      "[CV]  criterion=mse, max_depth=None, min_samples_leaf=10, min_samples_split=10, score=0.5029997648783718, total=  18.9s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   8 out of   8 | elapsed:  3.9min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=RandomForestRegressor(bootstrap=True, criterion='mse', max_depth=None,\n",
       "           max_features='auto', max_leaf_nodes=None,\n",
       "           min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "           min_samples_leaf=1, min_samples_split=2,\n",
       "           min_weight_fraction_leaf=0.0, n_estimators=10, n_jobs=1,\n",
       "           oob_score=False, random_state=None, verbose=0, warm_start=False),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'max_depth': [None], 'min_samples_split': [3, 10], 'min_samples_leaf': [3, 10], 'criterion': ['mse']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=4)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_val)\n",
    "# Truncate predictions between 0 and 1\n",
    "y_pred[y_pred < 0] = 0\n",
    "y_pred[y_pred > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.16860184158151342"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "!pip install xgboost\n",
    "from xgboost import XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multioutputregressor = MultiOutputRegressor(XGBRegressor(objective='reg:linear')).fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mor_pred = multioutputregressor.predict(X_val)\n",
    "# Truncate\n",
    "mor_pred[mor_pred > 1] = 1\n",
    "mor_pred[mor_pred < 0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse = math.sqrt(mean_squared_error(y_val, mor_pred))\n",
    "rmse # 0.16388598799139095"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = Ridge()\n",
    "\n",
    "param_grid = {'alpha': [25, 10, 5, 2.5, 1, 0.75, 0.5, 0.25, 0.1, 0.05, 0.025, 0.01, 0.001]}\n",
    "\n",
    "clf = GridSearchCV(clf, param_grid=param_grid, cv=2, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 13 candidates, totalling 26 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  26 out of  26 | elapsed:  8.4min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=2, error_score='raise',\n",
       "       estimator=Ridge(alpha=1.0, copy_X=True, fit_intercept=True, max_iter=None,\n",
       "   normalize=False, random_state=None, solver='auto', tol=0.001),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'alpha': [25, 10, 5, 2.5, 1, 0.75, 0.5, 0.25, 0.1, 0.05, 0.025, 0.01, 0.001]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=1)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_val)\n",
    "# Truncate predictions between 0 and 1\n",
    "y_pred[y_pred < 0] = 0\n",
    "y_pred[y_pred > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.15631172322618572"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rmse = math.sqrt(mean_squared_error(y_val, y_pred))\n",
    "rmse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensembling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset for S, W and K classifications.\n",
    "y_train_S = y_train[:,0:5]\n",
    "y_train_W = y_train[:, 5:9]\n",
    "y_train_K = y_train[:, 9:23]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Sentiment\n",
    "clf.fit(X_train, y_train_S)\n",
    "y_pred_S = clf.predict(X_val)\n",
    "# Truncate predictions between 0 and 1\n",
    "y_pred_S[y_pred_S < 0] = 0\n",
    "y_pred_S[y_pred_S > 1] = 1\n",
    "# rmse = math.sqrt(mean_squared_error(y_val[:,0:5], y_pred_S))\n",
    "# rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict When\n",
    "clf.fit(X_train, y_train_W)\n",
    "y_pred_W = clf.predict(X_val)\n",
    "# Truncate predictions between 0 and 1\n",
    "y_pred_W[y_pred_W < 0] = 0\n",
    "y_pred_W[y_pred_W > 1] = 1\n",
    "# rmse = math.sqrt(mean_squared_error(y_val[:,5:9], y_pred_W))\n",
    "# rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict Kind\n",
    "clf.fit(X_train, y_train_K)\n",
    "y_pred_K = clf.predict(X_val)\n",
    "# Truncate predictions between 0 and 1\n",
    "y_pred_K[y_pred_K < 0] = 0\n",
    "y_pred_K[y_pred_K > 1] = 1\n",
    "# rmse = math.sqrt(mean_squared_error(y_val[:,9:23], y_pred_K))\n",
    "# rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prediction = clf.predict(X_test)\n",
    "# Truncate predictions between 0 and 1\n",
    "test_prediction[test_prediction < 0] = 0\n",
    "test_prediction[test_prediction > 1] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = np.array(np.hstack([np.matrix(test['id']).T, test_prediction])) \n",
    "col = '%i,' + '%f,'*23 + '%f'\n",
    "np.savetxt('data/output.txt', prediction,col, delimiter=',')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train error: 0.14432603441695394\n"
     ]
    }
   ],
   "source": [
    "print('Train error: {0}'.format(np.sqrt(np.sum(np.array(np.array(clf.predict(X))-y)**2)/ (X.shape[0]*24.0))))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
